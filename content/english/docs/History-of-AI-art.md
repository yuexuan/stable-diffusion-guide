---
title: History of AI art
date: 2023-11-20T17:53:08+08:00
type: docs
weight: 1
prev: /
next: docs/infrastructure/
created: 2023-11-15T10:01:30+08:00
---

The developmental history of AI art doesn't lend itself to a grand narrative. It lacks the epic tales of national wars, the heart-wrenching stories of love novels, or the thrilling twists of detective thrillers. What it does have is a community of life-loving, science and technology enthusiasts, brimming with imagination. They have trodden the path of AI art, step by step, responding to real-life production needs, undergoing hundreds, if not thousands, of failures and iterations.

We express gratitude to those who failed, as they explored impractical directions in the pursuit of drawing with AI. Though they may not appear in the annals of history, they are worthy of appreciation.

## AARON, Ode to an Idealist

A British artist, [Harold Cohen](https://en.wikipedia.org/wiki/Harold_Cohen_(artist)), in the 1960s and 70s, created AARON, a system operated through traditional coding to produce paintings. Despite its limited output, AARON could create abstract works, such as the self-portrait below, reminiscent of an AI-era Van Gogh. <img alt="Another emotionally-aware portrait" loading="lazy" decoding="async" src="/images/Another emotionally-aware portrait.png" /> Without the advent of large AI models, we might still be at this stage. With Harold's departure, AARON also entered the pages of history. To explore more of AARON's artistic creations, check [Creative AI: The robots that would be painters](https://newatlas.com/creative-ai-algorithmic-art-painting-fool-aaron/36106/) and [harold-cohen-aaron](https://outland.art/harold-cohen-aaron/).

## The Marvelous Year 2012

2012 was a remarkable year, not because of the Mayan prediction of civilization's end (a disaster movie about 2012), but perhaps because it marked the true beginning of the AI era. From that year onward, enthusiasts in AI technology no longer had to fumble in the dark; they saw the only torch in the darkness, GPU + big data.

In that year, [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) and his students utilized NVIDIA's GPU to train their neural network model, [AlexNet](https://en.wikipedia.org/wiki/AlexNet). With the support of two GTX 580 graphics cards, training on 14 million images took just one week, and AlexNet claimed victory. Later acquired by Google, they released AlphaGo, gaining fame in just three years.

In the same year, members of the Google Brain team, including Jeff Dean and Andrew Ng, used deep learning to "recognize" cats in YouTube videos. <img alt="Another emotionally-aware portrait" loading="lazy" decoding="async" src="/images/learned to detect cats.png" /> Can you recognize it? This was a result of a $1 million investment, harnessing the power of 1,000 computers, 16,000 CPUs, and three days of effort.

For those interested, you can explore more in [2012: 180 Days that Changed Human Destiny](https://36kr.com/p/2421889040802823).

## 2015, Genesis of Machine Vision

In the previous round, Google emerged as the big winner. In this round, naturally, they were not willing to fall behind. In 2015, they open-sourced deep dream, a pioneering project in machine vision. Their implementation was quite simple—artistic processing of images. Take a look at the surreal result below: 
<img
      alt="Deep Dream Art"
      loading="lazy"
      decoding="async"
      src="/images/Deep Dream Art.png"
    />

You can explore the story behind it in [Unveiling the Past and Present of Google's Deep Dream](https://www.jiqizhixin.com/articles/2015-12-26-2).

In the year preceding the deep dream craze, 2014, Ian Goodfellow from the University of Montreal introduced the GAN (Generative Adversarial Network) model, unintentionally becoming the mainstream for image generation in machine vision. How about being surprised by a face that doesn't actually exist? 
<img
      alt="this person does not exist"
      loading="lazy"
      decoding="async"
      src="/images/thispersondoesnotexist.jpg"
    />

GAN operates by training two deep neural network models, a generator and a discriminator. The generator produces new data samples similar to real data, while the discriminator accurately distinguishes between fake samples generated by the generator and real data. During training, the generator strives to produce more realistic samples, while the discriminator improves its ability to differentiate between real and generated samples. These two models engage in adversarial and collaborative interactions, ultimately achieving high-quality data generation.

By learning from high-quality content, GANs can be used for denoising to some extent. Therefore, GANs are not limited to generating images; they can also generate videos, music, design drafts, and more. Building on the GAN model, enthusiasts have developed various popular architectures such as DCGAN, StyleGAN, BigGAN, StackGAN, Pix2pix, Age-cGAN, CycleGAN, and even created a [GAN ZOO](https://github.com/hindupuravinash/the-gan-zoo), each designed to tackle specific domain problems.

In the later stages of development, DeepFakes gained popularity, turning face-swapping into a captivating game that drew widespread participation.

Of course, GAN models have significant limitations, such as difficulty in improving resolution for generated content. For example, generating a good 512px image might work, but pushing to 1024px becomes challenging. Another limitation is the lack of fine control over generated content. Once we input a prompt, we're left waiting without the ability to control details like making the skin a bit whiter or adjusting the lighting. Nevertheless, in that era, GANs were irreplaceable.

If, and that's a big if, Google hadn't open-sourced the transformer model, and Microsoft hadn't introduced LoRA, there might not have been the later king, Stable Diffusion. We would still be stuck in a world dominated by GAN models, where every desired result would require a GAN-specific model—something beyond the reach of ordinary consumers and exclusive to AI practitioners.

If you're interested in the history of GAN development, you might want to read [The Development History of Generative Adversarial Networks (GANs)](https://zhuanlan.zhihu.com/p/63428113) and [Generative Adversarial Networks - The Story So Far](https://blog.floydhub.com/gans-story-so-far/).

The story of 2015 is not over yet. In the same year, the ImageNet image recognition competition, seemingly just beginning, was about to conclude. AI's recognition error rate had already surpassed that of humans. [Understanding the Tenfold Increase in Deep Learning in 2016](https://36kr.com/p/1721287491585)

Image classification continued, continually surpassing human capabilities until January 2021 when the OpenAI team (yes, the team famous for ChatGPT) open-sourced the cutting-edge CLIP (Contrastive Language-Image Pre-Training) deep learning model for image classification. Its contribution to the explosion of AIGC in the following year cannot be overstated.

## The Unrivaled King of Stable Diffusion

<img alt="Space Opera Theater" loading="lazy" decoding="async" src="/images/Space Opera Theater.png" />

If you frequently roam the realms of the internet, the image above is undoubtedly familiar to you. Yes, it's the winning piece, 'Space Opera Theater,' created by AI in an art competition at the Colorado State Fair.

The concept of AI-generated art seems to have permeated the entire world in an astonishing manner, especially for those who have yet to delve into the realm of AI.

As mentioned earlier, despite years of development, Generative Adversarial Networks (GANs) face persistent bottlenecks. At this juncture, some might wonder if an alternative approach is possible. Enter the Diffusion Model.

<img alt="Image addition noise" loading="lazy" decoding="async" src="/images/Image addition noise.png" />

The Diffusion Model involves continually adding noise to the original image using a Markov chain, resulting in a randomized noisy image. Subsequently, training neural networks to reverse this process, gradually restoring the random noise image to the original, grants the neural network the ability to generate images seemingly from scratch. Text-to-image generation involves treating descriptive text as noise, continually adding it to the original image, allowing the neural network to generate images from text.

The Diffusion Model simplifies the training process, requiring only a large number of images. Its generated image quality can reach impressive levels, and the results exhibit significant diversity. This is why the new generation of AI possesses an almost unbelievable 'imagination.' Within just two years of its inception, the Diffusion Model has brought AI-generated art to a usable level.

While the Diffusion Model was proposed in 2015, it wasn't until openAI introduced [DALL-E](https://openai.com/blog/dall-e/) that it gained traction in the industry. Despite DALL-E not being open source and the initial product's performance not being particularly surprising or exciting, it set the direction for the entire AI art community: CLIP + Diffusion.

As a result, various diffusion models from different sources have emerged, vying for the future, even if it's just a name.

In August 2022, [stability.ai](https://stability.ai/) open-sourced [Stable Diffusion](https://github.com/CompVis/stable-diffusion). Yes, it's here. For the development history of Stable Diffusion, refer to [StableDiffusion Model Development History](https://www.cnblogs.com/chester-cs/p/17411578.html).

Commercial products riding on the fame of Stable Diffusion, like [NovelAI](https://novelai.net/), need no further elaboration.

Finally, adding to the allure of 'Space Opera Theater,' the software used is [Mid Djourney](https://www.midjourney.com/home), which is also based on diffusion at its core.

So, what are you waiting for? Let's embark on our Stable Diffusion learning journey.